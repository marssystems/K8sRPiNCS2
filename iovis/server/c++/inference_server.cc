#include <iostream>
#include <string>
#include <fstream>

#include <grpcpp/grpcpp.h>

#include "inference_service.grpc.pb.h"

using grpc::Server;
using grpc::ServerBuilder;
using grpc::ServerContext;
using grpc::Status;
using grpc::ServerReader;
using grpc::StatusCode;

using inferer::Inferer;
using inferer::Request;
using inferer::Reply;


#include <gflags/gflags.h>
#include <functional>
#include <iostream>
#include <fstream>
#include <random>
#include <string>
#include <memory>
#include <vector>
#include <time.h>
#include <limits>
#include <chrono>
#include <algorithm>
#include <sstream>

#include <inference_engine.hpp>
#include <ext_list.hpp>

using namespace InferenceEngine;

#include <opencv2/opencv.hpp>

#include <cstdlib>

class OCVReader
{
private:
	cv::Mat m_Img;
	size_t m_Size = 0;
	size_t m_Height = 0;
	size_t m_Width = 0;
	std::shared_ptr<unsigned char> m_Data;

public:
	OCVReader(const std::vector<char> &pImgBuffer)
	{
		m_Img = imdecode(cv::Mat(pImgBuffer), 1);

		m_Size = 0;

		if (m_Img.empty()) {
			return;
		}

		m_Size = m_Img.size().width * m_Img.size().height * m_Img.channels();
		m_Width = m_Img.size().width;
		m_Height = m_Img.size().height;
	}

	virtual ~OCVReader()
	{
	}

	size_t size() const
	{
		return m_Size;
	}

	size_t width() const { return m_Width; }

	size_t height() const { return m_Height; }

	void Release() noexcept
	{
		delete this;
	}

	std::shared_ptr<unsigned char> getData(int pWidth = 0, int pHeight = 0)
	{
		cv::Mat resized(m_Img);
		if (pWidth != 0 && pHeight != 0) {
			int iw = m_Img.size().width;
			int ih = m_Img.size().height;
			if (pWidth != iw || pHeight != ih) {
				std::cout << "Image is resized from (" << iw << ", " << ih << ") to (" << pWidth << ", " << pHeight << ")" << std::endl;
			}
			cv::resize(m_Img, resized, cv::Size(pWidth, pHeight));
		}

		size_t size = resized.size().width * resized.size().height * resized.channels();
		m_Data.reset(new unsigned char[size], std::default_delete<unsigned char[]>());
		for (size_t id = 0; id < size; ++id) {
			m_Data.get()[id] = resized.data[id];
		}
		return m_Data;
	}
};

#define C_COMP_DEV "MYRIAD"
#define C_NETWORK "face-detection-adas-0001.xml"
#define C_WEIGHTS "face-detection-adas-0001.bin"

class KInferenceEngine
{
private:
	InputInfo::Ptr m_InputInfo;
	InferRequest m_InferRequest;

	std::string m_ImgInputName, m_ImgInfoInputName;
	std::string m_OutputName;

	InferencePlugin plugin;

	int m_MaxProposalCount;
	int m_ObjectSize;

	std::string path_iovino;
	std::string path_runtime1;
	std::string path_runtime2;
	std::string path_models;

public:
	KInferenceEngine()
	{
		m_InputInfo = NULL;
		m_MaxProposalCount = 0;
		m_ObjectSize = 0;

		path_models = std::getenv("INTEL_OPENVINO_MODELS_DIR");
		path_iovino = std::getenv("INTEL_OPENVINO_DIR");
		path_runtime1 = path_iovino + "/deployment_tools/inference_engine/lib/armv7l/";
		path_runtime2 = path_iovino + "/deployment_tools/inference_engine/samples/build/armv7l/Release/lib/";

		std::cout << path_iovino << std::endl << path_runtime1 << std::endl << path_runtime2 << std::endl;
	}

	~KInferenceEngine() {}

	int Initialize()
	{
		try
		{
			// --------------------------- 3. Load Plugin for inference engine -------------------------------------
			std::cout << "Loading plugin" << std::endl;
			plugin = PluginDispatcher({path_iovino, path_runtime1, path_runtime2, ".", "" }).getPluginByDevice(C_COMP_DEV);

			// --------------------------- 4. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
			std::cout << "Loading network files." << std::endl;
			CNNNetReader networkReader;
			/** Read network model **/
			networkReader.ReadNetwork(path_models+C_NETWORK);

			/** Extract model name and load weights **/
			networkReader.ReadWeights(path_models+C_WEIGHTS);
			CNNNetwork network = networkReader.getNetwork();
			// -----------------------------------------------------------------------------------------------------

			// --------------------------- 5. Prepare input blobs --------------------------------------------------
			std::cout << "Preparing input blobs" << std::endl;

			/** Taking information about all topology inputs **/
			InputsDataMap inputsInfo(network.getInputsInfo());

			/** SSD network has one input and one output **/
			if (inputsInfo.size() != 1 && inputsInfo.size() != 2) throw std::logic_error("Sample supports topologies only with 1 or 2 inputs");

			std::cout << "Inputs size is " << inputsInfo.size() << std::endl;

			m_InputInfo = inputsInfo.begin()->second;

			SizeVector inputImageDims;
			/** Stores input image **/

			/** Iterating over all input blobs **/
			for (auto & item : inputsInfo) {
				/** Working with first input tensor that stores image **/
				if (item.second->getInputData()->getTensorDesc().getDims().size() == 4) {
					m_ImgInputName = item.first;

					std::cout << "Batch size is " << std::to_string(networkReader.getNetwork().getBatchSize()) << std::endl;

					/** Creating first input blob **/
					Precision inputPrecision = Precision::U8;
					item.second->setPrecision(inputPrecision);
				}
				else if (item.second->getInputData()->getTensorDesc().getDims().size() == 2) {
					m_ImgInfoInputName = item.first;

					Precision inputPrecision = Precision::FP32;
					item.second->setPrecision(inputPrecision);
					if ((item.second->getTensorDesc().getDims()[1] != 3 && item.second->getTensorDesc().getDims()[1] != 6) ||
						item.second->getTensorDesc().getDims()[0] != 1) {
						throw std::logic_error("Invalid input info. Should be 3 or 6 values length");
					}
				}
			}

			std::cout << "Image input name: " << m_ImgInputName << std::endl;
			std::cout << "Image info input name: " << m_ImgInfoInputName << std::endl;
			// -----------------------------------------------------------------------------------------------------

			// --------------------------- 6. Prepare output blobs -------------------------------------------------
			std::cout << "Preparing output blobs" << std::endl;

			OutputsDataMap outputsInfo(network.getOutputsInfo());

			DataPtr outputInfo;
			for (const auto& out : outputsInfo) {
				if (out.second->creatorLayer.lock()->type == "DetectionOutput") {
					m_OutputName = out.first;
					outputInfo = out.second;
				}
			}

			if (outputInfo == nullptr) {
				throw std::logic_error("Can't find a DetectionOutput layer in the topology");
			}

			const SizeVector outputDims = outputInfo->getTensorDesc().getDims();

			m_MaxProposalCount = outputDims[2];
			m_ObjectSize = outputDims[3];

			if (m_ObjectSize != 7) {
				throw std::logic_error("Output item should have 7 as a last dimension");
			}

			if (outputDims.size() != 4) {
				throw std::logic_error("Incorrect output dimensions for SSD model");
			}

			/** Set the precision of output data provided by the user, should be called before load of the network to the plugin **/
			outputInfo->setPrecision(Precision::FP32);

			std::cout << "Output name: " << m_OutputName << std::endl;
			// -----------------------------------------------------------------------------------------------------

			// --------------------------- 7. Loading model to the plugin ------------------------------------------
			std::cout << "Loading model to the plugin" << std::endl;

			ExecutableNetwork executable_network = plugin.LoadNetwork(network, {});
			// -----------------------------------------------------------------------------------------------------

			// --------------------------- 8. Create infer request -------------------------------------------------
			m_InferRequest = executable_network.CreateInferRequest();
			// -----------------------------------------------------------------------------------------------------
		}
		catch (const std::exception& error) {
			std::cout << error.what() << std::endl;
			return 1;
		}
		catch (...) {
			std::cout << "Unknown/internal exception happened." << std::endl;
			return 1;
		}

		std::cout << "Inference engine initialized." << std::endl;

		return 0;
	}  // int Initialize()

	std::string Infer(const std::vector<char> &pImgBuffer)
	{
		std::stringstream p_Result;

		try {
			OCVReader ocvReader(pImgBuffer);

			// --------------------------- 9. Prepare input --------------------------------------------------------
			std::shared_ptr<unsigned char> imageData(ocvReader.getData(m_InputInfo->getTensorDesc().getDims()[3], m_InputInfo->getTensorDesc().getDims()[2]));
			int imageWidth = ocvReader.width();
			int imageHeight = ocvReader.height();

			/** Creating input blob **/
			Blob::Ptr imageInput = m_InferRequest.GetBlob(m_ImgInputName);

			/** Filling input tensor with images. First b channel, then g and r channels **/
			size_t num_channels = imageInput->getTensorDesc().getDims()[1];
			size_t image_size = imageInput->getTensorDesc().getDims()[3] * imageInput->getTensorDesc().getDims()[2];

			unsigned char* data = static_cast<unsigned char*>(imageInput->buffer());

			/** Iterate over all pixel in image (b,g,r) **/
			for (size_t pid = 0; pid < image_size; pid++) {
				/** Iterate over all channels **/
				for (size_t ch = 0; ch < num_channels; ++ch) {
					/**          [images stride + channels stride + pixel id ] all in bytes            **/
					data[ch * image_size + pid] = imageData.get()[pid*num_channels + ch];
				}
			}
			// -----------------------------------------------------------------------------------------------------

			// --------------------------- 10. Do inference ---------------------------------------------------------
			std::cout << "Start inference" << std::endl;

			typedef std::chrono::high_resolution_clock Time;
			typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;
			typedef std::chrono::duration<float> fsec;

			double total = 0.0;
			/** Start inference & calc performance **/
			auto t0 = Time::now();
			m_InferRequest.Infer();
			auto t1 = Time::now();
			fsec fs = t1 - t0;
			ms d = std::chrono::duration_cast<ms>(fs);
			total += d.count();
			// -----------------------------------------------------------------------------------------------------

			// --------------------------- 11. Process output -------------------------------------------------------
			std::cout << "Processing output blobs" << std::endl;

			const Blob::Ptr output_blob = m_InferRequest.GetBlob(m_OutputName);
			const float* detection = static_cast<PrecisionTrait<Precision::FP32>::value_type*>(output_blob->buffer());

			std::vector<int> boxes;
			std::vector<int> classes;

			std::stringstream p_Proposals;

			p_Result << "[" << std::endl << "\t{" << std::endl << "\t\t\"detections\": {" << std::endl << "\t\t\t\"rectangle\": [" << std::endl;

			/* Each detection has image_id that denotes processed image */
			for (int curProposal = 0; curProposal < m_MaxProposalCount; curProposal++) {
				float image_id = detection[curProposal * m_ObjectSize + 0];
				if (image_id < 0) {
					break;
				}

				float label = detection[curProposal * m_ObjectSize + 1];
				float confidence = detection[curProposal * m_ObjectSize + 2];
				float xmin = detection[curProposal * m_ObjectSize + 3] * imageWidth;
				float ymin = detection[curProposal * m_ObjectSize + 4] * imageHeight;
				float xmax = detection[curProposal * m_ObjectSize + 5] * imageWidth;
				float ymax = detection[curProposal * m_ObjectSize + 6] * imageHeight;

				if (confidence > 0.5) {
					/** Drawing only objects with >50% probability **/
					classes.push_back(static_cast<int>(label));
					boxes.push_back(static_cast<int>(xmin));
					boxes.push_back(static_cast<int>(ymin));
					boxes.push_back(static_cast<int>(xmax - xmin));
					boxes.push_back(static_cast<int>(ymax - ymin));


					p_Proposals << "\t\t\t\t{" << std::endl;
					p_Proposals << "\t\t\t\t\"label\": " << static_cast<int>(label) << "," << std::endl;
					p_Proposals << "\t\t\t\t\"probability\": " << static_cast<float>(confidence) << "," << std::endl;
					p_Proposals << "\t\t\t\t\"width\": " << static_cast<int>(xmin) << "," << std::endl;
					p_Proposals << "\t\t\t\t\"height\": " << static_cast<int>(ymin) << "," << std::endl;
					p_Proposals << "\t\t\t\t\"left\": " << static_cast<int>(xmax - xmin) << "," << std::endl;
					p_Proposals << "\t\t\t\t\"top\": "  << static_cast<int>(ymax - ymin) << std::endl;
					p_Proposals << "\t\t\t\t}," << std::endl;
				}
			}
			// -----------------------------------------------------------------------------------------------------
			
			std::string temp = p_Proposals.str();
			temp = temp.substr(0, temp.size()-2);  // remove the endl and comma
			p_Result << temp << std::endl;

			p_Result << "\t\t\t]" << std::endl;
			p_Result << "\t\t}," << std::endl;
			p_Result << "\t\"time\": " << total << std::endl;
			p_Result << "\t}" << std::endl;
			p_Result << "]" << std::endl;
		}
		catch (const std::exception& error) {
			std::cout << error.what() << std::endl;
			return "";
		}
		catch (...) {
			std::cout << "Unknown/internal exception happened." << std::endl;
			return "";
		}

		return p_Result.str();
	}  // std::string Infer()

};  // class KInferenceEngine

class RPCService final : public Inferer::Service
{
private:
	KInferenceEngine *ie = NULL;

public:
	RPCService(KInferenceEngine *p) { ie = p; }

	Status infer(ServerContext* context, ServerReader<Request>* stream, Reply* response) override
	{
		Request buffer;
		std::vector<char> imgBuffer;
		while (stream->Read(&buffer))
			std::copy(buffer.content().begin(), buffer.content().end(), std::back_inserter(imgBuffer));

		std::string p_Result = ie->Infer(imgBuffer);

		response->set_message(p_Result);

		return Status::OK;
	}
};

int main(int argc, char** argv)
{
	try
	{
		KInferenceEngine ie;
		ie.Initialize();

		std::string server_address("0.0.0.0:50051");
		RPCService service(&ie);

		ServerBuilder builder;
		// Listen on the given address without any authentication mechanism.
		builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());
		// Register "service" as the instance through which we'll communicate with
		// clients. In this case it corresponds to an *synchronous* service.
		builder.RegisterService(&service);
		// Finally assemble the server.
		std::unique_ptr<Server> server(builder.BuildAndStart());
		std::cout << "Server listening on " << server_address << std::endl;

		// Wait for the server to shutdown. Note that some other thread must be
		// responsible for shutting down the server for this call to ever return.
		server->Wait();
	}
	catch (const std::exception& error) {
		std::cout << error.what() << std::endl;
		return 1;
	}
	catch (...) {
		std::cout << "Unknown/internal exception happened." << std::endl;
		return 1;
	}

	std::cout << "Execution successful" << std::endl;
	return 0;
}
